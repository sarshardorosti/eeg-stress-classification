{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import variables\n",
    "importlib.reload(variables)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import variables as v  \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define parameters (these are unchanged)\n",
    "data_type = \"ica_filtered\"\n",
    "test_type = \"Arithmetic\"\n",
    "epochs = 20\n",
    "sfreq = 1\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate = 0.00001\n",
    "patience = 10\n",
    "min_delta = 0.001\n",
    "model_save_dir = \"epoch_model\"\n",
    "num_layers = 4\n",
    "num_neurons = [64,32,32,64]\n",
    "dropout_rate = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load the dataset\n",
    "def load_dataset(data_type=data_type, test_type=test_type):\n",
    "    # Ensure test and data types are valid\n",
    "    assert test_type in v.TEST_TYPES\n",
    "    assert data_type in v.DATA_TYPES\n",
    "\n",
    "    # Check for data-type/test-type compatibility\n",
    "    if data_type == \"ica_filtered\" and test_type not in v.TEST_TYPES:\n",
    "        print(\"Data of type\", data_type, \"does not have test type\", test_type)\n",
    "        return None\n",
    "\n",
    "    # Get the directory path for the data type\n",
    "    dir_path = getattr(v, f\"DIR_{data_type.upper()}\")\n",
    "    dataset = []\n",
    "\n",
    "    # Define key for accessing data in MATLAB files\n",
    "    if data_type == \"raw\":\n",
    "        data_key = 'Data'\n",
    "    elif data_type == \"wt_filtered\" or data_type == \"ica_filtered\":\n",
    "        data_key = 'Clean_data'\n",
    "\n",
    "    # Load data from files\n",
    "    for filename in os.listdir(dir_path):\n",
    "        try:\n",
    "            if test_type in filename:\n",
    "                filepath = os.path.join(dir_path, filename)\n",
    "                mat_contents = scipy.io.loadmat(filepath)\n",
    "                data = mat_contents[data_key]\n",
    "                dataset.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {filename}: {e}\")\n",
    "    return np.array(dataset)\n",
    "\n",
    "# Function to load labels\n",
    "def load_labels():\n",
    "    # Check if the labels file exists\n",
    "    if not os.path.exists(v.LABELS_PATH):\n",
    "        print(f\"Labels file not found at {v.LABELS_PATH}\")\n",
    "        return None\n",
    "\n",
    "    # Load and process labels\n",
    "    labels_df = pd.read_excel(v.LABELS_PATH)\n",
    "    labels_df = labels_df.rename(columns=v.COLUMNS_TO_RENAME)\n",
    "    labels_df = labels_df.iloc[1:]\n",
    "    expected_columns = list(v.COLUMNS_TO_RENAME.values())\n",
    "\n",
    "    # Check if all expected columns are present\n",
    "    if not all(col in labels_df.columns for col in expected_columns):\n",
    "        print(\"One or more expected columns are missing in the labels file.\")\n",
    "        return None\n",
    "\n",
    "    # Convert labels to binary (greater than 5 becomes 1, else 0)\n",
    "    labels = (labels_df > 5).astype(float)\n",
    "    return labels\n",
    "\n",
    "# Function to format labels\n",
    "def format_labels(labels, test_type=test_type, epochs=1):\n",
    "    # Ensure test type is valid and labels are loaded\n",
    "    assert test_type in v.TEST_TYPES\n",
    "    if labels is None or not len(labels):\n",
    "        print(\"Labels are empty or not loaded properly.\")\n",
    "        return None\n",
    "    if not isinstance(labels, pd.DataFrame):\n",
    "        print(\"Labels are not a DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    # Check for missing columns in labels\n",
    "    trial_columns = v.TEST_TYPE_COLUMNS[test_type]\n",
    "    missing_columns = [col for col in trial_columns if col not in labels.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Missing columns in labels: {missing_columns}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Format labels for each trial\n",
    "    formatted_labels = []\n",
    "    for trial in trial_columns:\n",
    "        formatted_labels.extend(labels[trial].tolist())\n",
    "    return np.repeat(formatted_labels, epochs)\n",
    "\n",
    "# Function to split data into epochs\n",
    "def split_data(data, sfreq=sfreq):\n",
    "    n_trials, n_channels, n_samples = data.shape\n",
    "    n_epochs = n_samples // sfreq\n",
    "    epoched_data = data.reshape(n_trials, n_channels, n_epochs, sfreq)\n",
    "    return epoched_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Custom class derived from Dataset for managing EEG data\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=num_layers, num_neurons=num_neurons, dropout_rate=dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_features = input_size if i == 0 else num_neurons[i - 1]\n",
    "            out_features = num_neurons[i]\n",
    "            layers.append(torch.nn.Linear(in_features, out_features))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(dropout_rate))\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "        self.fc_final = torch.nn.Linear(num_neurons[-1], output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.fc_final(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class EEG_CNN(torch.nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super(EEG_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(num_channels, 64, kernel_size=10, stride=1)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=10, stride=1)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)   \n",
    "        \n",
    "        self.flat = torch.nn.Flatten()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(5760, 128)\n",
    "        self.dropout1 = torch.nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.dropout2 = torch.nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(64, 64)\n",
    "        self.dropout3 = torch.nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc4 = torch.nn.Linear(64, num_classes)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "# Function for training the model for one epoch\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.reshape(-1, 32, 64, 50)\n",
    "        pred = model(X)\n",
    "        \n",
    "        pred = pred.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(pred, y)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_items += y.size(0)\n",
    "        correct += (pred.round() == y).sum().item()\n",
    "    return total_loss / len(dataloader), correct / total_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for validating the model for one epoch\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.reshape(-1, 32, 64, 50)\n",
    "            pred = model(X)\n",
    "            pred = pred.squeeze(1)\n",
    "            total_loss += loss_fn(pred, y).item()\n",
    "            total_items += y.size(0)\n",
    "#             print(y.size(0), y.shape, pred.shape)\n",
    "            correct += (pred.round() == y).sum().item()\n",
    "    return total_loss / len(dataloader), correct / total_items\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to track gradients for validation\n",
    "        for data, labels in test_loader:\n",
    "            data = data.reshape(-1, 32, 64, 50)\n",
    "            outputs = model(data)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs.round() == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Class for implementing early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=patience, min_delta=min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "# Unit test functions\n",
    "def test_data_loading():\n",
    "    assert load_dataset(data_type, test_type) is not None\n",
    "    assert load_labels() is not None\n",
    "\n",
    "def test_model_initialization():\n",
    "    model = SimpleNN(102400, v.N_CLASSES, num_layers=num_layers, num_neurons=num_neurons)\n",
    "    assert model is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run unit tests\n",
    "test_data_loading()\n",
    "test_model_initialization()\n",
    "\n",
    "# Main execution\n",
    "dataset = load_dataset(data_type, test_type)\n",
    "labels = load_labels()\n",
    "if dataset is None or labels is None:\n",
    "    print(\"Data or labels could not be loaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Process the data and labels\n",
    "formatted_labels = format_labels(labels, test_type, epochs=1)\n",
    "epoched_data = split_data(dataset, sfreq)\n",
    "flattened_data = epoched_data.reshape(epoched_data.shape[0], -1)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(flattened_data, formatted_labels, test_size=0.5)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.75)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = EEGDataset(train_data, train_labels)\n",
    "val_dataset = EEGDataset(val_data, val_labels)\n",
    "test_dataset = EEGDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model initialization\n",
    "input_size = flattened_data.shape[1]\n",
    "output_size = v.N_CLASSES\n",
    "# model = SimpleNN(102400, v.N_CLASSES, num_layers=num_layers, num_neurons=num_neurons)\n",
    "model = EEG_CNN(32, v.N_CLASSES)\n",
    "\n",
    "\n",
    "# Loss function, optimizer, and scheduler\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=1)\n",
    "early_stopper = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or clear the model save directory\n",
    "if os.path.exists(model_save_dir):\n",
    "    shutil.rmtree(model_save_dir)\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Training and validation code (assuming other necessary functions are defined)\n",
    "models_saved_count = 0\n",
    "last_saved_epoch = 0\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_one_epoch(train_loader, model, criterion, optimizer)\n",
    "    val_loss, val_acc = validate(val_loader, model, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Validation\n",
    "#     val_loss = validate(val_loader, model, criterion)\n",
    "#     val_accuracy = evaluate_model(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Save the model for each epoch in the specified folder\n",
    "    epoch_model_path = os.path.join(model_save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "    torch.save(model.state_dict(), epoch_model_path)\n",
    "    models_saved_count += 1\n",
    "    last_saved_epoch = epoch + 1  # Update the last saved epoch\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if False and early_stopper(val_loss):\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Function to load a trained SimpleNN model\n",
    "def load_model(model_path):\n",
    "    model = EEG_CNN(32, 1)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the last saved model and evaluate\n",
    "model_path_to_load = os.path.join(model_save_dir, f\"model_epoch_{last_saved_epoch}.pth\")\n",
    "loaded_model = load_model(model_path_to_load)\n",
    "\n",
    "# Assuming test_loader is already defined\n",
    "if test_dataset is None or test_labels is None:\n",
    "    print(\"Test data or labels could not be loaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# test_loss = validate(test_loader, loaded_model, criterion)\n",
    "test_accuracy = evaluate_model(loaded_model, test_loader)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plotting training, validation loss and validation accuracy\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot for training and validation loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for training and validation accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg-stress-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
